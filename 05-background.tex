This chapter provides the theoretical foundation and practical motivation for
approximate timestamping in on-disk database systems. It examines the
fundamental principles of concurrency control in transactional systems, then
traces the evolution of storage technology that has reshaped performance
bottlenecks, introduces a representative approximate data structure, and finally
presents the compelling case for why approximate timestamping represents a
necessary innovation.

\section{Concurrency Control in Transactional Systems}
\label{sec:background:cc}

Concurrency control is the set of mechanisms that ensure multiple transactions
can execute simultaneously while maintaining data consistency. To understand why
this is crucial, this section first examines the fundamental properties that all
transactional systems must guarantee.

\subsection{The ACID Properties and the Need for Concurrency Control}
\label{sec:background:acid}

Transactional database systems are built upon four fundamental guarantees,
collectively known as the ACID properties~\cite{haerder1983principles}:
Atomicity, Consistency, Isolation, and Durability. These properties ensure that
database operations behave reliably even when multiple users access the system
simultaneously.

\textbf{Atomicity} ensures that each transaction is treated as a single,
indivisible unit. Either all operations within a transaction complete
successfully, or none of them do. If any part of a transaction fails, the entire
transaction is rolled back, leaving the database in its original state.

\textbf{Consistency} guarantees that a transaction brings the database from one
valid state to another. All data integrity rules and constraints are maintained,
ensuring that the database remains in a correct state after each transaction.

\textbf{Durability} ensures that once a transaction commits, its effects are
permanent and will survive system failures. Committed data is stored
persistently and can be recovered even after crashes or power outages.

\textbf{Isolation} is perhaps the most complex property and the primary focus of
concurrency control. It ensures that concurrent transactions do not interfere
with each other, making it appear as if transactions execute one at a time, even
when they actually run simultaneously. This property is crucial because without
proper isolation, concurrent transactions could see partial results from each
other, leading to inconsistent data states.

Concurrency control mechanisms are specifically designed to enforce the
isolation property. They ensure that concurrent transactions result in a system
state that is equivalent to some serial execution of those transactions—a
property called \emph{serializability}. This means that even though transactions
may run at the same time, the final result is the same as if they had executed
one after another in some order.

\subsection{A Taxonomy of Concurrency Control Protocols}
\label{sec:background:taxonomy}

Concurrency control protocols can be broadly categorized into three main
families, each with distinct approaches to managing concurrent access to data.
Understanding these approaches is essential for appreciating the trade-offs
involved in different concurrency control strategies.

\subsubsection{Lock-Based Concurrency Control}

Lock-based concurrency control operates on the principle of preventing conflicts
before they occur. These protocols assume that conflicts are likely and take
proactive measures to avoid them by controlling access to data items.

\textbf{Two-Phase Locking (2PL)}~\cite{eswaran1976notions} is the most widely
used lock-based protocol. It operates in two distinct phases. During the
\textit{Growing Phase}, a transaction can acquire locks on data items but cannot
release any locks. This phase continues as the transaction reads and writes
data. Once the transaction enters the \textit{Shrinking Phase}, it can only
release locks and cannot acquire new ones. This phase continues until the
transaction completes.

This two-phase rule ensures that no other transaction can access a data item
between the time when a transaction reads it and when it writes to it,
preventing certain types of inconsistencies. This property, combined with the
requirement that all locks are held until commit, enables 2PL to prevent
cascading aborts: once a transaction commits, no other transaction that read its
data needs to be aborted. However, these benefits come at a cost: managing locks
requires significant computational resources, including maintaining lock tables
and handling lock requests, resulting in high CPU overhead. When transactions
wait for each other's locks, deadlocks can occur, requiring detection and
resolution mechanisms. These deadlock risks are particularly problematic in
high-contention scenarios, where many transactions may be blocked waiting for
locks, reducing overall system throughput and limiting parallelism.

To address these deadlock issues, practical implementations often use policies
like \emph{no-wait}~\cite{DBLP:conf/sigmod/RenFA16,DBLP:conf/usenix/TangE18},
where transactions immediately abort if they cannot acquire a needed lock,
rather than waiting and potentially creating deadlocks.


\subsubsection{Optimistic Concurrency Control}

Optimistic concurrency control (OCC)~\cite{kung1981optimistic} takes a
fundamentally different approach to handling concurrent transactions. Rather
than using locks to prevent conflicts from occurring, OCC assumes that conflicts
are rare and permits transactions to proceed in parallel without synchronization
during most of their execution.

A typical OCC protocol consists of three phases. During the \textit{Read Phase},
a transaction reads data and performs computations, keeping track of the data
items it accesses but making changes only to private, local copies. When the
transaction is ready to commit, the \textit{Validation Phase} begins, where the
system checks whether the data the transaction read has been modified by other
transactions since it started. If so, the transaction must abort and is
typically retried. If validation succeeds, the \textit{Write Phase} applies the
transaction's changes to the database atomically.

There is no blocking on locks, so deadlocks cannot occur. Transactions operate
freely during the read phase, maximizing parallelism and providing high
concurrency. When conflicts are rare, aborts are infrequent and performance is
high, making OCC efficient for low-contention workloads. However, in workloads
with frequent conflicting accesses, many transactions may be aborted and
retried, reducing throughput due to high abort rates under contention. The
process of checking for conflicts at commit time incurs computational overhead,
especially as the number of transactions scales. Careful protocol design is
required to prevent subtle anomalies like write skew in some OCC
implementations.

Overall, OCC is well-suited to environments where transactions are short and
conflicts are rare, such as many in-memory and read-mostly workloads.


\subsubsection{Timestamp-Based Concurrency Control}
\label{sec:background:timestamp-based-cc}

Timestamp-based protocols use temporal ordering to determine the serialization
order of transactions, assigning each transaction a unique timestamp that
determines when it should appear to execute relative to other transactions.

\textbf{Strict Timestamp Ordering (STO)}~\cite{bernstein1987concurrency} is a
traditional good timestamp-based protocol. Each transaction receives a timestamp
when it starts, and this timestamp determines its position in the serialization
order. When accessing a data item, the transaction compares its timestamp
against the item's read and write timestamps. If the access would violate the
timestamp ordering rules, the transaction immediately aborts.

STO uses both read and write timestamps for each data item, allowing higher
concurrency than single-timestamp approaches. A transaction can read an item if
its timestamp is greater than the item's write timestamp, and can write an item
if its timestamp is greater than the item's read timestamp.

\textbf{Multi-Version Timestamp Ordering (MVTO)}~\cite{reed1983mvto} extends
timestamp ordering by maintaining multiple versions of each data item. This
allows read-only transactions to access older, consistent snapshots without
conflicting with concurrent write transactions. MVTO is particularly effective
for read-heavy workloads where many transactions need to read data while fewer
transactions perform updates.

However, MVTO introduces significant overhead in terms of storage space (for
maintaining multiple versions) and garbage collection (for removing old versions
that are no longer needed).

\textbf{TicToc (Optimistic Timestamp Ordering)}~\cite{yu2016tictoc} represents
an advanced approach that combines the benefits of timestamp ordering with
optimistic execution. Instead of assigning timestamps at transaction start,
TicToc dynamically chooses a commit timestamp during the validation phase. By
selecting a timestamp that is guaranteed to be valid with respect to all
accessed data, TicToc minimizes the time window for potential conflicts,
significantly reducing abort rates compared to traditional optimistic protocols.

\section{The Evolution of On-Disk Storage Systems}
\label{sec:background:storage}

This section examines how advances in storage technology have fundamentally
transformed the performance characteristics of database systems. The section
begins by examining how the performance bottleneck has shifted from I/O latency
to CPU overhead, then traces the evolution of storage media from hard disk
drives to modern NVMe SSDs, and finally introduces CXL as an emerging technology
that bridges storage and memory semantics.

\subsection{The Shifting Performance Landscape}
\label{sec:background:performance-shift}

For decades, mechanical storage (HDDs) imposed millisecond-scale latency that
dominated end-to-end performance, masking other sources of overhead such as the
CPU cost of concurrency control. With the advent of solid-state storage—and
especially NVMe SSDs—access latencies dropped to tens of microseconds, exposing
non-I/O bottlenecks, most notably CPU overhead from concurrency control
protocols.

\textbf{Hard Disk Drives (HDDs)} represent the traditional storage technology
that dominated database systems for decades. Their performance is fundamentally
limited by mechanical factors: the seek time required to move the read/write
head to the correct track, and the rotational latency needed for the desired
data to rotate under the head. These mechanical limits yield very low random I/O
performance, typically only 100--200 input/output operations per second (IOPS)
with access times of several milliseconds~\cite{samsung2013ssd}. HDDs remain in
use due to low cost and high capacity.

\textbf{SATA Solid-State Drives (SSDs)} eliminated mechanical delays but were
constrained by the legacy SATA III interface, which was designed for
single-queue spinning disks. The SATA interface caps bandwidth at approximately
600 MB/s, and the AHCI protocol limits parallelism. Despite these constraints,
SATA SSDs achieve latencies in the hundreds of microseconds and can support up
to approximately 100,000 IOPS—a significant improvement over
HDDs~\cite{samsung2013ssd}.

\textbf{NVMe Solid-State Drives (SSDs)} represent the current state-of-the-art,
designed from the ground up for flash memory. NVMe utilizes the high-bandwidth,
low-latency PCIe bus and supports massive parallelism with up to 64,000 command
queues. Modern Gen4 NVMe drives can achieve~\cite{crucial-t700-review}
throughput exceeding 7 GB/s, over 1 million IOPS, and latencies in the tens of
microseconds.


\subsection{Compute Express Link (CXL): Accelerating SSDs with Memory-Semantic IO}
\label{sec:background:cxl}

Compute Express Link (CXL)~\cite{das2024introduction,cxl-consortium} is a
cache-coherent protocol atop PCIe that unifies CPUs, accelerators, and memory
devices into a single memory-centric domain. It enables shared, coherent
load/store access between host and devices, reducing synchronization and
data-movement overheads, and it scales via CXL switches to attach additional
devices without redesign. In particular, Type-3 (memory-expansion) devices
expose host-managed device memory (HDM) that the CPU accesses directly with
ordinary loads and stores~\cite{das2024introduction}.

In the context of Type-3 memory expansion, CXL can accelerate SSDs by bringing
them into the cache-coherent domain, overcoming fundamental limitations of
traditional PCIe storage~\cite{jung2022hello,yang2023overcoming}. Historically,
while PCIe devices could map internal memory through Base Address Registers
(BARs), the host had to treat such accesses as non-cacheable, preventing use of
CPU caches and degrading performance. With CXL, load/store requests to a
CXL-attached SSD's address space—mapped into the host's cacheable system
memory—become cacheable. Incorporating PCIe storage into the coherent hierarchy
bridges block semantics to byte-addressable, memory-compatible semantics and
substantially lowers latency: projections report up to 129.5$\times$ lower
latency than PCIe-based memory expanders due to host cache
usage~\cite{jung2022hello}, and CXL-enabled SSDs can serve 68–91\% of memory
requests in under a microsecond~\cite{yang2023overcoming}.

\subsection{Modern Write-Optimized Data Structures}
\label{sec:background:write-optimized}

The evolution of storage technology has driven the development of new data
structures designed to exploit the characteristics of solid-state storage.

Log-Structured Merge (LSM) trees~\cite{OneillChGaOn96} and similar
write-optimized structures have become increasingly popular for on-disk storage
systems. These structures are designed to maximize write performance by batching
and organizing writes efficiently.

A key property of these structures is the Query/Insert
Asymmetry~\cite{bender2015introduction}: they can ingest new data (writes)
orders of magnitude faster than they can service random queries (reads). For
example, SplinterDB~\cite{splinterdb}, which uses a write-optimized structure
called a \bets~\cite{BrodalFa03}, can perform over 2 million random insertions
per second but only about 500,000 random queries per second.

This asymmetry is crucial for understanding the performance implications of
different concurrency control approaches. When timestamp-based protocols store
timestamps in write-optimized structures, they may need to perform a slow query
operation to fetch existing timestamps before each fast write operation,
effectively negating the performance benefits of the write-optimized structure.

\section{Approximate Data Structures}
\label{sec:background:approximate-data-structures}

Approximate data structures are a class of data structures that provide an
approximation of the data stored in the data structure. This approximation is
used to reduce the memory requirements of the data structure.

\subsection{Count-Min Sketch}
\label{sec:background:count-min-sketch}

The count-min sketch (CMS)~\citep{CormodeMu05} was originally proposed as a data
structure for providing approximate counts of the occurrences of each item in a
data stream.  The CMS maintains a two-dimensional table $A$ of height $h$ and
width $w$. Upon observing an item $x$ in the stream, the CMS increments
$A[i][h_i(x)]$ for each row $i$, where $h_i$ are all hash functions. The CMS
estimates the number of occurrences of an item $x$ as $CMS[x] =
min_iA[i][h_i(x)]$.  The CMS obviously never underestimates the count of an
item, and guarantees that
\[
  \Pr[CMS[x] \leq c_x + n/w ] \geq 1 - e^{-d},
\]
where $c_x$ is the true count of $x$ and $n$ is the total number of
observations added to the CMS.

\section{Motivation: The Case for Approximate Timestamping}
\label{sec:background:motivation}

The preceding discussions on concurrency control and storage systems converge to
create a compelling argument for why approximate timestamping represents a
necessary innovation for modern database systems.

\subsection{The Concurrency Control Bottleneck Revisited}
\label{sec:background:cc-bottleneck}

With the dramatic reduction in I/O latency provided by NVMe SSDs, the CPU
overhead inherent in lock-based protocols like 2PL is no longer masked by
storage delays. This overhead now stands out as a primary factor limiting system
throughput, making computationally efficient protocols like timestamp ordering
theoretically more appealing.

The computational efficiency of timestamp-based protocols comes from their
ability to make concurrency control decisions based on simple timestamp
comparisons rather than complex lock management. However, applying these
protocols directly to on-disk systems introduces new challenges.

\subsection{The Prohibitive Cost of Naive On-Disk Timestamps}
\label{sec:background:disk-timestamp-cost}

The critical flaw in applying timestamp ordering directly to on-disk systems
lies in the storage requirements for timestamps. Storing per-key timestamps on
disk creates a new, I/O bottleneck that can negate the performance benefits of
timestamp-based protocols.

This problem is particularly acute when combined with the Query/Insert Asymmetry
of write-optimized data structures. A fast write operation in an LSM-tree
becomes a slow read-modify-write cycle when timestamps are stored on disk, as
the system must first perform a slow query to fetch the key's existing
timestamps before the fast write can proceed. This disproportionately degrades
write performance and eliminates the primary advantage of write-optimized
structures.

\subsection{The Impracticality of a Fully In-Memory Solution}
\label{sec:background:memory-impractical}

The obvious alternative of storing all timestamps in RAM is not viable for
large-scale database systems. Real-world deployments, such as Facebook's
RocksDB~\cite{rocksdb} instances, demonstrate that the memory required for
timestamps alone could consume a substantial portion of available RAM.

For example, with key-value pairs averaging less than 100
bytes~\cite{DBLP:conf/fast/CaoDVD20,AtikogluXuFr12}, storing two 8-byte
timestamps per pair would require approximately 16\% of the data size in
additional memory. However, many production systems operate with RAM-to-disk
ratios of less than 5\%, sometimes as little as
3\%~\cite{DBLP:conf/fast/CaoDVD20}. Even in scenarios with larger key-value
pairs or more abundant RAM, timestamps would still consume a significant portion
of memory that could otherwise be used for caching or other purposes.

\subsection{Case Study: The Promise and the Pitfall}
\label{sec:background:benchmarks}

\begin{figure}[!t]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tikzpicture}
            \begin{groupplot}[group style={group size=1 by 1,
                            /pgf/bar width=3.6pt},
                    height = 4cm,
                    width = 10cm,
                    ybar= 2*\pgflinewidth,
                    xtick=data,
                    xticklabels={
                            \shortstack{High-contention \\ Read-intensive},
                            \shortstack{High-contention \\ Write-intensive},
                            \shortstack{Medium-contention \\ Read-intensive},
                            \shortstack{Medium-contention \\ Write-intensive}
                        },
                    ticklabel style={font=\tiny},
                    ymajorgrids=true,
                    enlarge x limits = 0.15,
                    legend columns=1,
                    legend entries={{\tiny 2PL}, {\tiny OCC}, {\tiny STO-Disk}, {\tiny STO-Memory}, {\tiny STO-\sketchname}, {\tiny TicToc-Disk}, {\tiny TicToc-Memory}, {\tiny TicToc-\sketchname}},
                    area legend,
                    legend to name=grouplegend,
                ]

                % graph [1,1] high
                \nextgroupplot[YCSBThroughputBarChartPlot, ylabel={Goodput (KTPS)}]
                \addplot[2PL-BarStyle] %2PL
                coordinates {(0,17.477367)
                        (1,7.801877)
                        (2,44.415733)
                        (3,98.0184)};
                \addplot[KR-OCC-BarStyle] %KR-OCC
                coordinates {(0,16.476967)
                        (1,16.502733)
                        (2,12.110933)
                        (3,9.95364)};
                \addplot[STO-Disk-BarStyle] %STO-Disk
                coordinates {(0,30.2847)
                        (1,6.549243)
                        (2,41.467833)
                        (3,47.448567)};
                \addplot[STO-Memory-BarStyle] %STO-Memory
                coordinates {(0,38.878867)
                        (1,11.258033)
                        (2,48.277933)
                        (3,99.339033)};
                \addplot[STO-FPSketch-BarStyle] %STO-FPSketch
                coordinates {(0,38.5077)
                        (1,11.6077)
                        (2,46.990067)
                        (3,90.8817)};
                \addplot[TicToc-Disk-BarStyle] %TicToc-Disk
                coordinates {(0,49.263467)
                        (1,8.097707)
                        (2,29.168467)
                        (3,38.399333)};
                \addplot[TicToc-Memory-BarStyle] %TicToc-Memory
                coordinates {(0,140.330333)
                        (1,59.562333)
                        (2,48.216233)
                        (3,100.82)};
                \addplot[TicToc-FPSketch-BarStyle] %TicToc-FPSketch
                coordinates {(0,115.33)
                        (1,47.927533)
                        (2,45.608067)
                        (3,101.462)};
                \coordinate (top) at (rel axis cs:0,1);% coordinate at top of the first plot
                \coordinate (bot) at (rel axis cs:1,0);% coordinate at bottom of the last plot
            \end{groupplot}
            \coordinate (c) at ($(top)!.5!(bot)$);

            \node[right=1cm, yshift=-5pt] at (c -| current bounding box.east) {\ref{grouplegend}};
        \end{tikzpicture}
    } \caption[Highlights of goodputs for YCSB workloads]{Performance
    comparison of various concurrency control mechanisms across different YCSB
    workloads, demonstrating the promise of in-memory timestamp ordering and the
    pitfalls of disk-based approaches. (Running 120 threads on NVMe SSD)}
    \label{fig:motivation}
\end{figure}

\Cref{fig:motivation} evaluates YCSB workloads, modified to execute transactions
in parallel, spanning contention (high, medium) and access patterns (read- vs.
write-intensive). It reports goodput, the throughput of successfully committed
transactions, to discount wasted work from aborts. The evaluation compares 2PL,
OCC, and two timestamp-based methods (STO and TicToc), each under three
timestamp placement strategies: stored on disk, in an idealized in-memory exact
timestamp store, and the proposed approximate timestamp store (\sketchname with
32KiB). This bracketing (Disk vs. Memory) with \sketchname\ in between isolates
the core tradeoff—CPU efficiency of timestamp-based methods versus the I/O and
lookup costs of materializing per-key timestamps on disk—while holding hardware
constant. The observed goodput gaps motivate approximate timestamping to achieve
near-in-memory performance with a minimal memory footprint.


\paragraph{The Promise (TicToc-Memory):} When timestamps are stored in an idealized
in-memory hash table, advanced protocols like TicToc dramatically outperform
traditional 2PL and OCC. TicToc-Memory achieves the highest throughput across
all workloads, with particularly impressive performance in high-contention
scenarios, demonstrating the immense potential of timestamp-based schemes.

\paragraph{The Pitfall (TicToc-Disk):} When these same timestamps are stored on
disk, the I/O overhead becomes so crippling that performance collapses, often
falling far below that of 2PL and OCC, especially in write-intensive workloads.
The performance gap between Memory and Disk variants is particularly pronounced
for write-intensive workloads, where the Query/Insert Asymmetry has the greatest
impact.

\paragraph{The Opportunity:} This stark contrast between the in-memory and
on-disk variants perfectly frames the research problem. It establishes a clear
need for a solution that can deliver the high performance of the in-memory
approach without its prohibitive memory cost. The results from STO-\sketchname
and TicToc-\sketchname, which use the proposed approximate timestamping
technique, demonstrate that such a solution is achievable, setting the stage for
the detailed presentation of this approach in the following chapters.
