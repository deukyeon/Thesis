\section{The Enduring Challenge of Concurrency Control in On-Disk Databases} 

Achieving high-performance transaction processing in disk-based databases is
challenging because there is an inherent tension between ensuring data
consistency and maximizing performance. To safely execute multiple transactions
at once, databases rely on Concurrency Control (CC) techniques, which guarantee
that the outcome matches some serial order of
transactions~\cite{rocksdb,cassandra,mongodb,couchbase,redis,berkeleydb,lmdb}.
As a result, a great deal of research has focused on creating effective CC
mechanisms~\cite{yu2016tictoc,wolf2015evaluation,tu13silo,plor,polaris,2PLFS,chardonnay,bamboo}.
For decades, selecting an appropriate CC method has required system designers to
grapple with a difficult balance.

On one side, traditional lock-based methods like Two-Phase Locking
(2PL)~\cite{eswaran1976notions} are common. These methods do not need much
memory because locks are only needed for the data items being used at any
moment. However, lock-based methods can make the CPU do a lot of extra work,
because transactions need to get and hold locks, sometimes for a long time. This
holding of locks can force transactions to wait for each other, which reduces
how many can happen at the same time and causes performance problems, especially
when there are many transactions.

On the other side are timestamp-based methods, such as Strict Timestamp Ordering
(STO)~\cite{bernstein1987concurrency}, Multi-Version Timestamp Ordering
(MVTO)~\cite{reed1983mvto}, and newer approaches such as
TicToc~\cite{yu2016tictoc}. These methods use timestamps to determine the
serialization order of transactions, often achieving higher concurrency and
fewer aborted transactions than lock-based approaches. In traditional timestamp
ordering schemes like STO and MVTO, each transaction receives a timestamp when
it begins, which determines its position in the serialization order. When
accessing a data item, these schemes compare the transaction's timestamp against
the item's read and write timestamps: a transaction can read an item only if its
timestamp is greater than the item's write timestamp, and can write an item only
if its timestamp is greater than the item's read and write timestamps. If an
access would violate these ordering rules, the transaction immediately aborts.

TicToc represents a more advanced approach: rather than assigning timestamps at
transaction start, it employs a data-driven protocol that computes commit
timestamps lazily during validation (the final check for conflicts before a
transaction commits). By examining the actual timestamps of all accessed data
items when a transaction is ready to commit, TicToc can dynamically find valid
logical orderings that allow it to successfully commit transactions that
traditional timestamp-based protocols would abort. This flexibility is key to
avoiding aborts: for example, a transaction that commits physically after
another transaction can receive a logical commit timestamp that places it before
the other transaction in the serialization order, computed based on the
timestamps of keys accessed by both transactions. In TicToc, each data item has
a valid range of timestamps [write timestamp ($wts$), read timestamp ($rts$)],
and a transaction's commit timestamp must lie within the valid ranges of all
accessed data items. This ability to decouple physical commit order from logical
timestamp order ultimately reduces conflicts and improves concurrency.

However, the timestamp-based methods face a significant challenge in disk-based
databases: accessing timestamps on disk creates substantial I/O overhead. When
these timestamps are stored on disk, even simple operations become expensive.
For example, in a STO system~\cite{bernstein1987concurrency}, reading a data
item requires writing its read timestamp back to disk, while updating a row
requires first reading the item's existing timestamps before performing the
update. This metadata overhead transforms what should be fast, simple operations
into slower read-modify-write cycles, significantly reducing system performance.

One might think that modern, fast storage devices like NVMe solid-state drives
(SSDs)~\cite{crucial-t700-review} would solve this problem, but they do not.
NVMe SSDs are much faster than older drives, but they actually make this
trade-off more obvious. With such fast storage, the delays caused by lock-based
protocols become more pronounced, since the system is often waiting for the CPU
rather than the storage. At the same time, if timestamp metadata is kept on
disk, the SSDs' high speed is wasted dealing with many small metadata I/O
operations. As a result, no matter which method is chosen—CPU-heavy locks or
I/O-heavy timestamps—the system ends up limited by either CPU or storage. This
makes it hard for designers to fully use the power of new hardware.

This fundamental trade-off raises a deeper question: why must timestamps persist
on disk at all? The answer lies in the requirements of timestamp-based
concurrency control protocols. When a new transaction begins and accesses a key,
it must check that key's timestamp to determine whether the transaction can
proceed or must abort to maintain serializability. Moreover, high-performance
timestamp-based CC methods including TicToc~\cite{yu2016tictoc} dynamically find
a logical timestamp ordering during the validation phase based on the timestamps
of the keys being accessed, resulting in higher concurrency by committing more
transactions than traditional timestamp-based CC methods. In both cases,
timestamps cannot be discarded simply because no transaction is currently
accessing a key, as future transactions will need this information to make
correct concurrency control decisions. However, maintaining exact timestamps for
all keys in memory is prohibitively impractical for large databases, creating a
significant storage space overhead.

The key insight that resolves this dilemma is that, while exact timestamps are
required for keys currently being accessed by active transactions, it is
sufficient to maintain only an upper bound (a value that is guaranteed to be
greater than or equal to the true timestamp) for inactive keys. Timestamp-based
CC protocols compare transaction timestamps against key timestamps to make abort
decisions. If inactive keys' timestamps were to be completely forgotten, the
critical information needed to enforce serializability when new transactions
access these keys would be lost. However, by maintaining an overapproximation (a
value that may be larger than the true timestamp but never smaller) instead of
exact values, correctness is preserved while dramatically reducing memory
requirements. This overapproximation may cause harmless extra transaction aborts
(when the system assumes a key's timestamp is larger than it actually is), but
it will not violate database correctness.

Yet storing approximate timestamps creates an inherent compromise between
memory footprint and accuracy. Allocating more space for timestamp storage keeps
its values close to the precise timestamps, while shrinking the structure (e.g.,
by collapsing timestamps of inactive keys into a single shared counter such as
the last commit timestamp) rapidly widens the gap between approximate and
precise values. The CC protocol must then assume larger timestamps for inactive
keys, triggering more conservative abort decisions and reducing the number of
committed transactions.

This compromise particularly affects TicToc, whose advantage depends on having
access to past timestamp information during validation. As noted earlier, TicToc
requires this historical information to dynamically determine correct commit
orderings. When keys become inactive and their timestamps are quickly
overapproximated, TicToc loses access to the precise historical data it needs.
Instead, it must base its validation decisions on overapproximated timestamps,
which constrains its ability to find optimal commit orderings. For example,
consider a transaction that accesses a key whose true timestamps, $wts=100$ and
$rts=160$, but has been overapproximated to $wts=150$. TicToc must assume the
key was last modified at time 150. If the transaction could have committed with
any timestamp in the range $[101, 160]$ when using precise timestamps, the
overapproximation reduces this viable range to $[151, 160]$, eliminating 50
possible commit timestamps.

This conservative behavior erodes TicToc's advantage: as overapproximation
widens, TicToc's ability to leverage past timestamp information diminishes,
making its dynamic timestamp selection less effective and degrading the number
of committed transactions (shown in \Cref{fig:sketch_size}). This fundamental
tension between memory efficiency and timestamp precision motivates the need for
a new approach that can maintain sufficiently accurate timestamps for
timestamp-based CC protocols while avoiding the prohibitive memory overhead of
storing exact timestamps for all keys.

\section{\sketchname: A New Approach for Timestamp-Based CC on Disk}

To address this challenge, the first main contribution presents a new approach
for efficient timestamp-based concurrency control in disk-based databases. This
approach is called \sketchname. The main idea behind \sketchname is to decompose
traditional timestamp-based concurrency control mechanisms into two components:
(1) the CC protocol itself (such as STO~\cite{bernstein1987concurrency} or
TicToc~\cite{yu2016tictoc}), and (2) an approximate storage system for
timestamps, which is called \sketchname.

A key observation enables this decomposition: in reality, only a small
fraction of the database keys are being accessed by currently active
transactions at any given time. While the system must maintain exact timestamps
for active keys to ensure correctness, the vast majority of keys are inactive at
any moment. Building on the insight that overapproximation preserves
correctness, \sketchname can store approximate upper bounds for these inactive
keys with minimal memory overhead, dramatically reducing the storage
requirements compared to maintaining exact timestamps for all keys.

Based on these observations, \sketchname is designed as a memory-efficient and
fast data structure, with two key components:

\begin{enumerate}
    \item \textbf{A Hash Table (Foveated Region):} This part holds the exact
    timestamps for the keys that are currently active in transactions. A simple
    counting mechanism ensures that as long as a key is needed by a transaction,
    its timestamp remains exact.
    \item \textbf{A Sketch (Peripheral Region):} This component, inspired by
    structures like the Count-Min Sketch~\cite{CormodeMu05}, keeps approximate
    upper bounds for inactive keys. When a key becomes inactive (its reference
    count falls to zero), its last timestamp is moved from the hash table to
    this sketch.
\end{enumerate}

Thanks to this hybrid structure, many slow and costly disk accesses for
timestamps are replaced by very fast memory operations. In effect, \sketchname
gives nearly the same performance as if every timestamp could be kept in fast
memory, but it needs much less RAM (for example, only 32KiB for an 80GB database
in experiments).

With the problem of metadata I/O removed, high-performance concurrency control
protocols can be adapted for disk-based databases. Experiments with the
SplinterDB key-value store~\cite{splinterdb} (detailed in \Cref{sec:eval})
demonstrate that \sketchname has a large impact: using \sketchname, advanced
protocols like TicToc achieve speeds up to 14$\times$ faster than traditional
2PL and KR-OCC~\cite{kung1981optimistic}, and up to 5.9$\times$ faster than
standard disk-based timestamp methods on some workloads. These results
demonstrate that \sketchname successfully resolves the tension between memory
efficiency and timestamp precision, enabling high-performance timestamp-based
concurrency control on disk-based databases.

\section{Evaluating \sketchname Across the Storage Spectrum}

While \sketchname works well on modern NVMe SSDs, these results raise a deeper
research question: Is using approximate, in-memory metadata always useful, or is
its benefit only clear for today's fast storage technologies like NVMe SSDs?
Separating the concurrency control logic from storage performance seems
powerful, but this requires evaluation across all types of storage, from
traditional, slower devices to the most modern memory-like devices.

The second main contribution addresses this issue directly. It presents a
thorough analytical study of the \sketchname approach across a variety of
storage devices. The analysis explores how useful the approach remains when the
performance gap between memory and disk access changes dramatically. The goal is
to determine if the key idea behind \sketchname is broadly applicable and
future-proof, or if it is mainly a solution for today's specific hardware.

\subsection{A Taxonomy of Storage Performance}

To make the analysis concrete, the following summarizes the important technical
differences among the storage devices considered. These differences are not only
in their performance measurements, but also in how each device's architecture
interacts with the database and concurrency control systems.

\begin{itemize}
    \item \textbf{Hard Disk Drives (HDDs):} HDDs are mechanical devices limited
    by moving parts, such as spinning platters and moving arms. As a result,
    their access times are dominated by seek and rotation delays, leading to
    average latencies in the 5-20 millisecond range. Their performance for
    random I/O is poor, typically capable of only 100-200
    IOPS~\cite{samsung2013ssd}.
    \item \textbf{SATA SSDs:} These solid-state drives are much faster than HDDs
    since they have no moving parts. However, common SATA SSDs are limited by
    the older SATA interface and AHCI protocol, which were designed for spinning
    disks and do not take full advantage of flash memory's capability to handle
    many requests in parallel. This puts their latency in the 100-500
    microsecond range, and throughput is about 600 MB/s~\cite{samsung2013ssd}.
    \item \textbf{NVMe SSDs:} The NVMe standard connects SSDs directly to the
    CPU using the PCIe bus and supports high degrees of parallelism (up to
    64,000 command queues), leading to another big increase in speed. Newer NVMe
    SSDs can achieve latencies as low as 20-100 microseconds, random read IOPS
    above 1 million, and sequential throughput over 7,000
    MB/s~\cite{crucial-t700-review}. This was the platform where \sketchname was
    first tested.
    \item \textbf{CXL-based SSDs:} Compute Express Link (CXL)-based SSDs is the
    newest type of storage that tries to remove the distinction between memory
    and storage~\cite{yang2023overcoming,zhong2025oasis}. These devices connect
    through PCIe but use the CXL.mem protocol, meaning the CPU can read their
    contents through direct memory instructions, like main memory. This
    eliminates traditional storage software overhead and the device looks like a
    memory-only node. Access is not as fast as regular DRAM, but much faster
    than NVMe SSDs, with latency in the single-digit microsecond range.
\end{itemize}

These major differences are shown in \Cref{tab:storage-characteristics}, which
provides the performance data used in the analyses that follow.

\begin{table}[h]
\centering
\caption{Comparative Performance Characteristics of Storage
Media~\cite{samsung2013ssd, crucial-t700-review, yang2023overcoming}}
\label{tab:storage-characteristics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
 & HDD & SATA SSD & NVMe SSD & CXL-based SSD \\
\midrule
\midrule
Latency & 5-20 ms & 100-500 µs & 20-100 µs & 1-10 µs \\
\midrule
Sequential Throughput & ~150-250 MB/s & ~600 MB/s & ~7,000 MB/s & ~16 GB/s (PCIe 5.0 x4) \\
\midrule
Interface/Protocol & SATA / ATA & SATA / AHCI & PCIe / NVMe & PCIe / CXL.mem \\
\midrule
Primary Bottleneck & Mechanical (Seek/Rotation) & Interface/Protocol & CPU / Concurrency & Interconnect / Memory Controller \\
\bottomrule
\end{tabular}%
}
\end{table}

\subsection{Slow Storage Environments}

First, consider using \sketchname in HDDs and SATA SSDs. In these systems, each
random I/O operation is extremely slow, measured in milliseconds for HDDs and
hundreds of microseconds for SATA SSDs. Traditional approaches for disk-based
databases, such as 2PL and OCC, have been the standard choice for these
environments because they avoid the metadata I/O overhead. On slow storage
devices, their CPU overheads are largely hidden by the dominant cost of disk
I/O, making these methods acceptable despite their limitations. A simple
approach that stores timestamps on disk would therefore be severely limited by
slow disk I/O delays, making high-performance concurrency control protocols
impractical. \sketchname's ability to keep timestamps in memory becomes even
more critical and outperforms traditional approaches in these environments by
eliminating the need for slow disk I/O operations.

\subsection{Fast Storage Environments}

Second, consider emerging CXL-based SSDs, which represent a fundamental shift in
storage architecture. CXL (Compute Express Link) enables byte-addressable
storage that appears to the CPU as a remote NUMA node, allowing direct memory
access without the traditional I/O stack. CXL-based SSDs offer latencies in the
low microsecond range—much faster than block devices, yet still incurring remote
memory communication overhead. Since CXL-based SSDs are still emerging
technologies, the evaluation simulates their performance rather than using
physical hardware.

In this fast storage environment, \sketchname's purpose shifts from avoiding
slow disk I/O to maintaining critical metadata in fast local memory. If
timestamps were stored directly on the CXL-based SSD, each access would incur
several microseconds of remote memory communication latency. Even simple
protocols like TicToc require many such accesses per transaction, meaning these
small delays accumulate and significantly impact performance.

\sketchname solves this problem by acting as an application-specific cache for
concurrency metadata, with its hash table storing the exact timestamps for
active keys close to the CPU in local DRAM. This ensures frequently used
metadata is always available with the lowest possible delay. While \sketchname
avoids the performance bottlenecks that arise from remote memory access, it
introduces its own overhead from hash table operations and memory allocation.
On fast storage, these overheads become visible relative to storage access
costs, yet \sketchname still provides substantial benefits by keeping metadata
in fast local memory and enabling timestamp-based protocols to significantly
outperform traditional approaches.

This analysis shows that \sketchname is more than just a new data structure; it
is, in fact, a flexible method for managing metadata in a tiered memory/storage
world. Its value becomes even greater as the distinction between memory and
storage fades.

\section{Thesis Statement}

The main thesis is: \textbf{\textit{Approximate, in-memory metadata management
enables high-performance transaction processing for disk-based databases.}} Its
advantages do not depend on any single storage technology; instead, they scale
with the basic, long-lasting gap between local computation and remote storage
access, allowing advanced concurrency control protocols to reach nearly optimal
performance across all storage types. First, the design, implementation, and
evaluation of \sketchname using modern NVMe SSDs demonstrates this. Then, an
analytical study of storage devices from slow, mechanical drives to emerging
high-speed, memory-like storage shows its wider value and ongoing relevance.

\section{Dissertation Outline}

This dissertation builds on the following publication:

Deukyeon Hwang, Alex Conway, Carlos Garcia-Alvarado, Jun Yuan, Naama
Ben-David, Rob Johnson, and Adriana Szekeres. 2025. Focus! Fast On-disk
Concurrency-control Using Sketches. \textit{Proc. ACM Manag. Data} 3, 6 (SIGMOD),
Article 328 (December 2025), 27 pages. \url{https://doi.org/10.1145/3769793}




The dissertation is organized as follows:
\begin{description}[leftmargin=2.5em, labelindent=0em, style=nextline]
    \item[Chapter \ref{chapter:two}: Background and Motivation] 
    This chapter provides a full background on timestamp-based concurrency
    control protocols, modern storage technologies, and the performance features
    that motivate the research.

    \item[Chapter \ref{chapter:three}: \sketchname]
    This chapter presents the design and implementation of \sketchname, which is
    the first main contribution. It describes the method for approximate
    timestamp storage, the hybrid hash table and sketch data structure, and how
    it integrates with STO, MVTO, and TicToc protocols. It also provides a
    thorough experimental evaluation of \sketchname\ on a state-of-the-art NVMe
    SSD, comparing its performance to traditional concurrency control methods
    and ideal baselines across various workloads.

    \item[Chapter \ref{chapter:four}: Evaluating \sketchname Across the Storage Spectrum]
    This chapter presents the second primary contribution: a detailed analysis
    of the \sketchname\ paradigm across legacy (HDD, SATA SSD) and new (CXL-SSD)
    storage technologies, examining its impact and evolving role.

    \item[Chapter \ref{chapter:five}: Related Work]
    This chapter reviews related research on disk-based concurrency control,
    approximate data structures in databases and other areas.

    \item[Chapter \ref{chapter:six}: Conclusion and Future Work]
    This chapter summarizes the dissertation's contributions, restates the main
    thesis, and outlines possible directions for future study.
\end{description}
